{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZmvogQoTihm"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lk97wP0jQ8rJ",
    "outputId": "135581b3-838c-4697-d51a-8ea17b24bd7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers\n",
    "from transformers import DistilBertTokenizerFast \n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.text import  viterbi_decode\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFGd86lXTuaI"
   },
   "source": [
    "# Distilbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576,
     "referenced_widgets": [
      "f77f065f61c84f62ba15ea1b7cce3ce8",
      "80d8f4ba6f7645979f0bdc9c98213caf",
      "a5376145079d4d9c9b7f0263357e2fb8",
      "46aab4ce1a6d464ba185e6b239a578d1",
      "609bd6baa1304bb7b0c23839f1deca3a",
      "fc1960698a93486d964106034f31a66f",
      "c6b1983abf6248a58dd2a4b87c148e77",
      "9ad4c3bacfb9465685a5911c5a871cfe"
     ]
    },
    "id": "0_PXPI9AUKJE",
    "outputId": "a3f5d421-209e-4f6e-c0c8-318bd8f9afc9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77f065f61c84f62ba15ea1b7cce3ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distilbert (TFDistilBertMainLay ((None, 50, 768),)   66362880    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_slice1 (TensorFlowO (None, 1, 768)       0           distilbert[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_slice2 (TensorFlowO (None, 49, 768)      0           distilbert[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 768)          0           tf_op_layer_slice1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D (Conv1D)                 (None, 49, 6)        4614        tf_op_layer_slice2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cls_output (Dense)              (None, 8)            6152        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sentence_output (Softmax)       (None, 49, 6)        0           Conv1D[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 66,373,646\n",
      "Trainable params: 0\n",
      "Non-trainable params: 66,373,646\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_intents =  np.array( ['analyze_transactions' , 'balances' , 'bank_related' , 'chatbot_related' , 'check_credit_card_details' , \n",
    "                             'check_loan_details','greetings','transactions_intent'] )\n",
    "\n",
    "id2tag = { 0 : 'o' , 1 : 's-id', 2 : 'e-id', 3 : 's-product', 4 : 'e-product' , 5 : 'na' }\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "bert_model = tf.keras.models.load_model('/content/drive/My Drive/AAI/models/bert_model.h5')\n",
    "bert_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8vO9VaoTtfk"
   },
   "outputs": [],
   "source": [
    "\n",
    "def bert_model_out( text ):\n",
    "\n",
    "    '''\n",
    "    This function applies bert model trained on input queries of a banking chatbot to classify the intent and output ner tags at the same time.\n",
    "    Types of intents : 'analyze_transactions' , 'balances' , 'bank_related' , 'chatbot_related' , 'check_credit_card_details' , 'check_loan_details' , 'greetings','transactions_intent'\n",
    "    Types of tags : 'o','s-id','e-id','s-product','e-product','na' \n",
    "\n",
    "    Inputs to function : a single sentence in str format\n",
    "    Outputs w.r.t. index :\n",
    "    0 - intent_predictions , \n",
    "    1 - ner_predictions , \n",
    "    2 - word_vectors , \n",
    "    3 - sentence_vector    \n",
    "    '''\n",
    "\n",
    "    ner_predictions = []\n",
    "    word_vectors = []\n",
    "\n",
    "    # remove extra spaces from input text and split sentence to words\n",
    "    re.sub('  +' , ' ' , text)\n",
    "    text = [text.split(' ')]\n",
    "\n",
    "    # pretrained Distilbert wordpiece tokenizer with padding and truncation at specified max_length\n",
    "    encoding = tokenizer(text , is_pretokenized = True , return_offsets_mapping=True , padding='max_length' , truncation=True , max_length=50)\n",
    "\n",
    "    # 3 outputs from tokenizer - input ids , attention_mask and offset mapping from wordpiece segmentation\n",
    "    input_ids = np.array(encoding.input_ids)\n",
    "    attention_mask = np.array(encoding.attention_mask)\n",
    "    offset_mapping = encoding.offset_mapping\n",
    "\n",
    "    # input ids and attention mask inputted to pretrained model to get classifier and ner probability distributions \n",
    "    # along with output vector representation of each input token (CLS , SEP , PAD tokens included)\n",
    "    cls , sentence , vectors = bert_model([input_ids , attention_mask])\n",
    "\n",
    "    # vector output corresponding to CLS token is used as sentence vector\n",
    "    sentence_vector = vectors[0,0,:]\n",
    "\n",
    "    # argmax to find intent with highest probability\n",
    "    intent_predictions = unique_intents[tf.math.argmax(cls , axis = 1).numpy()][0] \n",
    "\n",
    "    # if we take sentence example - \"Pay Ramesh 1000\" (after tokenization we get ['pay' , 'Ram' , '##es' , '##sh' , '1000'])\n",
    "    # below stretch of words helps deal with offset words (Ramesh --> Ram + ##es + ##sh) from wordpiece segmentation\n",
    "    # it gives a dictionary 'start_end' where keys correspond to index of Ram in list above and values correspond to ##sh index\n",
    "    te1 = np.array([i for i,j in offset_mapping[0]]) > 0\n",
    "    te2 = np.roll( te1 , 1 )\n",
    "    starts = te1 & ~te2\n",
    "    ends = ~te1 & te2\n",
    "    starts = np.nonzero(starts)[0]\n",
    "    ends = np.nonzero(ends)[0]\n",
    "    start_end = dict(zip(starts,ends))\n",
    "\n",
    "    # loop on each token output\n",
    "    k = 1\n",
    "    for i in text[0]:\n",
    "\n",
    "        # if token index is in 'start_end' keys, we use the respective dictionary values and try to \"\"aggregate\"\" the distributed segments \n",
    "        # in terms of ner probability distribution and vectors corresponding to these segmented tokens input \n",
    "        if k in start_end.keys():\n",
    "\n",
    "            probs = sentence[0 , k-1 , : ]\n",
    "\n",
    "            # ner argmax for 1st word in segment is appended for reunited segment\n",
    "            ner = tf.math.argmax(probs).numpy()\n",
    "            ner = id2tag[ner]\n",
    "            ner_predictions.append(ner)\n",
    "\n",
    "            # word vectors are literally aggregated \n",
    "            word_vector = tf.reduce_mean(vectors[0 , k:start_end[k] , : ] , axis = 0)               \n",
    "            word_vectors.append(word_vector)\n",
    "\n",
    "            # new index is after last '##...' word\n",
    "            k = start_end[k]\n",
    "            \n",
    "        # if token index not in 'start_end' keys we simply append the argmax ner index and the generated vector for that token\n",
    "        else:\n",
    "\n",
    "            probs = sentence[0 , k-1 , : ]\n",
    "            \n",
    "            ner = tf.math.argmax(probs).numpy()\n",
    "            ner = id2tag[ner]\n",
    "            ner_predictions.append(ner)\n",
    "\n",
    "            word_vector = vectors[0 , k , :]\n",
    "            word_vectors.append(word_vector)\n",
    "\n",
    "            # new index is old + 1\n",
    "            k += 1\n",
    "            \n",
    "    return intent_predictions , ner_predictions , word_vectors , sentence_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "c5ljVkB4UUEa",
    "outputId": "438c3016-8e5a-4e37-d38d-c66557192a17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_fast.py:353: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions_intent \n",
      "\n",
      "['o', 's-id', 'na', 'e-id', 'o', 'o', 's-product'] \n",
      "\n",
      "[TensorShape([768]), TensorShape([768]), TensorShape([768]), TensorShape([768]), TensorShape([768]), TensorShape([768]), TensorShape([768])] \n",
      "\n",
      "(768,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "intent_predictions , ner_predictions , word_vectors , sentence_vector = bert_model_out('Pay Suryakanth Sharma with my credit card')\n",
    "\n",
    "print(intent_predictions ,'\\n')\n",
    "print(ner_predictions ,'\\n') \n",
    "print([i.shape for i in word_vectors] ,'\\n')\n",
    "print(sentence_vector.shape,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcPh2_lfQG-d"
   },
   "source": [
    "# GRU + CRF model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "yfRAZxqwRfsc",
    "outputId": "f95493fa-a61f-46cf-9486-994e8a9bd8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Word_Embedding_11 (Embedding)   (None, 50, 300)      9156600     input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 300)      406800      Word_Embedding_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 15000)        0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "cls_dense1 (Dense)              (None, 150)          2250150     reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_slice2_2 (TensorFlo (None, 49, 300)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "cls_output (Dense)              (None, 8)            1208        cls_dense1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D (Conv1D)                 (None, 49, 6)        1806        tf_op_layer_slice2_2[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 11,816,564\n",
      "Trainable params: 406,800\n",
      "Non-trainable params: 11,409,764\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_intents =  np.array( ['analyze_transactions' , 'balances' , 'bank_related' , 'chatbot_related' , 'check_credit_card_details' , \n",
    "                             'check_loan_details','greetings','transactions_intent'] )\n",
    "\n",
    "id2tag = { 0 : 'o' , 1 : 's-id', 2 : 'e-id', 3 : 's-product', 4 : 'e-product' , 5 : 'na' }\n",
    "\n",
    "with open('/content/drive/My Drive/AAI/models/crf_transition_params' , 'rb') as f:\n",
    "    transition_params = pickle.load(f)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "gru_model = tf.keras.models.load_model('/content/drive/My Drive/AAI/models/gru_model.h5')\n",
    "gru_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOhPp9lFPEuy"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gru_model_out( text ):\n",
    "\n",
    "    '''\n",
    "    This function applies gru + crf model trained on input queries of a banking chatbot to classify the intent and output ner tags at the same time.\n",
    "    Types of intents : 'analyze_transactions' , 'balances' , 'bank_related' , 'chatbot_related' , 'check_credit_card_details' , 'check_loan_details' , 'greetings','transactions_intent'\n",
    "    Types of tags : 'o','s-id','e-id','s-product','e-product','na' \n",
    "\n",
    "    Inputs to function : a single sentence in str format\n",
    "    Outputs w.r.t. index :\n",
    "    0 - intent_predictions , \n",
    "    1 - ner_predictions , \n",
    "    2 - word_vectors , \n",
    "    3 - sentence_vector    \n",
    "    '''\n",
    "\n",
    "    ner_predictions = []\n",
    "    word_vectors = []\n",
    "\n",
    "    # remove extra spaces from input text and split sentence to words\n",
    "    re.sub('  +' , ' ' , text)\n",
    "    text = [text.split(' ')]\n",
    "\n",
    "    # pretrained Distilbert wordpiece tokenizer with padding and truncation at specified max_length\n",
    "    encoding = tokenizer(text , is_pretokenized = True , return_offsets_mapping=True , padding='max_length' , truncation=True , max_length=50)\n",
    "\n",
    "    # 3 outputs from tokenizer - input ids , attention_mask and offset mapping from wordpiece segmentation\n",
    "    input_ids = np.array(encoding.input_ids)\n",
    "    attention_mask = np.array(encoding.attention_mask)\n",
    "    offset_mapping = encoding.offset_mapping\n",
    "\n",
    "    # input ids and attention mask inputtend to pretrained model to get classifier and ner probability distributions \n",
    "    # along with output vector representation of each input token (CLS , SEP , PAD tokens included)\n",
    "    cls , sentence , vectors = gru_model(input_ids)\n",
    "\n",
    "    # vector output corresponding to CLS token is used as sentence vector\n",
    "    sentence_vector = vectors[0,0,:]\n",
    "\n",
    "    # argmax to find intent with highest probability\n",
    "    intent_predictions = unique_intents[tf.math.argmax(cls , axis = 1).numpy()][0] \n",
    "\n",
    "    # virtebi_decode is algorithm which uses unary (from model output) and binary potentials (transition params) \n",
    "    # to predict whole ner sequence at once\n",
    "    target_sentence = viterbi_decode( sentence[0] , transition_params )[0]\n",
    "\n",
    "    # if we take sentence example - \"Pay Ramesh 1000\" (after tokenization we get ['pay' , 'Ram' , '##es' , '##sh' , '1000'])\n",
    "    # below stretch of words helps deal with offset words (Ramesh --> Ram + ##es + ##sh) from wordpiece segmentation\n",
    "    # it gives a dictionary 'start_end' where keys correspond to index of Ram in list above and values correspond to ##sh index\n",
    "    te1 = np.array([i for i,j in offset_mapping[0]]) > 0\n",
    "    te2 = np.roll( te1 , 1 )\n",
    "    starts = te1 & ~te2\n",
    "    ends = ~te1 & te2\n",
    "    starts = np.nonzero(starts)[0]\n",
    "    ends = np.nonzero(ends)[0]\n",
    "    start_end = dict(zip(starts,ends))\n",
    "\n",
    "    # loop on each token output\n",
    "    k = 1\n",
    "    for i in text[0]:\n",
    "\n",
    "        # if token index is in 'start_end' keys, we use the respective dictionary values and try to \"\"aggregate\"\" the distributed segments \n",
    "        # in terms of ner probability distribution and vectors corresponding to these segmented tokens input\n",
    "        if k in start_end.keys():\n",
    "\n",
    "            # ner argmax for 1st word in segment is appended for reunited segment\n",
    "            ner = target_sentence[ k-1 ]\n",
    "            ner = id2tag[ner]\n",
    "            ner_predictions.append(ner)\n",
    "\n",
    "            # word vectors are literally aggregated \n",
    "            word_vector = tf.reduce_mean(vectors[0 , k:start_end[k] , : ] , axis = 0)               \n",
    "            word_vectors.append(word_vector)\n",
    "\n",
    "            # new index is after last '##...' word\n",
    "            k = start_end[k]\n",
    "\n",
    "        # if token index not in 'start_end' keys we simply append the argmax ner index and the generated vector for that token    \n",
    "        else:\n",
    "\n",
    "            ner = target_sentence[ k-1 ]\n",
    "            ner = id2tag[ner]\n",
    "            ner_predictions.append(ner)\n",
    "\n",
    "            word_vector = vectors[0 , k , :]\n",
    "            word_vectors.append(word_vector)\n",
    "\n",
    "            # new index is old + 1\n",
    "            k += 1\n",
    "\n",
    "    return intent_predictions , ner_predictions , word_vectors , sentence_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "Jt0u6sWPQEE2",
    "outputId": "2443304b-0422-478e-aa27-27acb03a04c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions_intent \n",
      "\n",
      "['o', 's-id', 'na', 'o', 'o', 's-product'] \n",
      "\n",
      "[TensorShape([300]), TensorShape([300]), TensorShape([300]), TensorShape([300]), TensorShape([300]), TensorShape([300])] \n",
      "\n",
      "(300,) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_fast.py:353: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "intent_predictions , ner_predictions , word_vectors , sentence_vector = gru_model_out('Pay Suryakanth Sharma using credit card')\n",
    "\n",
    "print(intent_predictions ,'\\n')\n",
    "print(ner_predictions ,'\\n') \n",
    "print([i.shape for i in word_vectors] ,'\\n')\n",
    "print(sentence_vector.shape,'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intent_CLS+NER Inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "46aab4ce1a6d464ba185e6b239a578d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ad4c3bacfb9465685a5911c5a871cfe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c6b1983abf6248a58dd2a4b87c148e77",
      "value": " 232k/232k [00:01&lt;00:00, 179kB/s]"
     }
    },
    "609bd6baa1304bb7b0c23839f1deca3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "80d8f4ba6f7645979f0bdc9c98213caf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ad4c3bacfb9465685a5911c5a871cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5376145079d4d9c9b7f0263357e2fb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc1960698a93486d964106034f31a66f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_609bd6baa1304bb7b0c23839f1deca3a",
      "value": 231508
     }
    },
    "c6b1983abf6248a58dd2a4b87c148e77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f77f065f61c84f62ba15ea1b7cce3ce8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5376145079d4d9c9b7f0263357e2fb8",
       "IPY_MODEL_46aab4ce1a6d464ba185e6b239a578d1"
      ],
      "layout": "IPY_MODEL_80d8f4ba6f7645979f0bdc9c98213caf"
     }
    },
    "fc1960698a93486d964106034f31a66f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
